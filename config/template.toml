[base]
project_name = "PROJECT_NAME"  # also CLI arg
output_root = "output"
language = "da/en"  # also CLI arg

[preprocessing]
enabled = true
input_path = "PATH/TO/INPUT/*"  # also CLI arg
n_docs = -1   # also CLI arg
doc_type = "text/csv/tweets/infomedia"
metadata_fields = ["*"]

[preprocessing.extra]
# specific extra arguments for your preprocessor, e.g. context length for tweets or
# or field specification for CSVs

[docprocessing]
enabled = true
batch_size = 25
continue_from_last = true
triplet_extraction_method = "multi2oie/prompting"

[corpusprocessing]
enabled = true
embedding_model = "PATH_OR_NAME" # leave out for default model choice by language
dimensions = 100 # leave out to skip dimensionality reduction
n_neighbors = 15 # used for dimensionality reduction

[corpusprocessing.thresholds]  # leave out for automatic estimation
min_cluster_size = 3  # unused if auto_thresholds is true
min_samples = 3  # unused if auto_thresholds is true
min_topic_size = 5  # unused if auto_thresholds is true