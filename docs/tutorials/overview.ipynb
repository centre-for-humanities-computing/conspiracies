{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/centre-for-humanities-computing/conspiracies/blob/main/docs/tutorials/overview.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Coreference model\n",
    "A small use case of the coreference component in spaCy.\n",
    "\n",
    "This coreference component is trained on Twitter data annotated by the [Center for Humanities Computing](https://chc.au.dk), which will be released as a part of the DANSK dataset as well the [Danish coreference dataset](https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/coreference.md) by the Alexandra Institute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<conspiracies.docprocessing.coref.coref_component.CoreferenceComponent at 0x7fefb77b7640>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from conspiracies.docprocessing.coref import CoreferenceComponent\n",
    "\n",
    "nlp = spacy.blank(\"da\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(\"allennlp_coref\")  # download the model if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Do you see Julie over there? She is really into programming!\")\n",
    "\n",
    "assert isinstance(doc._.coref_clusters, list)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    assert isinstance(sent._.coref_clusters, list)\n",
    "    assert isinstance(sent._.coref_clusters[0], tuple)\n",
    "    assert isinstance(sent._.coref_clusters[0][0], int)\n",
    "    assert isinstance(sent._.coref_clusters[0][1], Span)\n",
    "    sent._.resolve_coref  # get resolved coref"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output a bit further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC LEVEL (Coref clusters)\n",
      "[(0, [Julie, She])]\n",
      "-----\n",
      "\n",
      "SPAN LEVEL (sentences)\n",
      "[(0, Julie)]\n",
      "[(0, She)]\n",
      "-----\n",
      "\n",
      "SPAN LEVEL (entities)\n",
      "\n",
      "Coref Entity: Julie \n",
      "Antecedent: Julie\n",
      "\n",
      "\n",
      "Coref Entity: She \n",
      "Antecedent: Julie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DOC LEVEL (Coref clusters)\")\n",
    "print(doc._.coref_clusters)\n",
    "print(\"-----\\n\\nSPAN LEVEL (sentences)\")\n",
    "for sent in doc.sents:\n",
    "    print(sent._.coref_clusters)\n",
    "print(\"-----\\n\\nSPAN LEVEL (entities)\\n\")\n",
    "for sent in doc.sents:\n",
    "    for i, coref_entity in sent._.coref_clusters:\n",
    "        print(f\"Coref Entity: {coref_entity} \\nAntecedent: {coref_entity._.antecedent}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headword Extraction\n",
    "A small use case of how to use the headword extraction component to extract headwords.\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `en_core_web_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download en_core_web_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "the Danish politician\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from conspiracies.docprocessing.headwordextraction.headwordextraction_component import contains_ents\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\n",
    "    \"heads_extraction\",\n",
    "    config={\"normalize_to_entity\": True, \"normalize_to_noun_chunk\": True},\n",
    ")\n",
    "\n",
    "doc = nlp(\"Mette Frederiksen is the Danish politician.\")\n",
    "heads_spans = []\n",
    "\n",
    "print(doc._.most_common_ancestor)\n",
    "the_danish = doc[3:5]\n",
    "print(the_danish._.most_common_ancestor)  # it normalizes to noun chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordpiece length normalization Extraction\n",
    "A small use case of how to use word piece length normalization to normalize the length of\n",
    "your texts in case you are applying transformer-based pipelines.\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `da_core_news_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download da_core_news_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load nlp (we don't recommend a trf based spacy model as it is too slow)\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "# load huggingface tokenizer - should be the same as the model you wish to apply later\n",
    "tokenizer_name = \"alexandrainst/da-sentiment-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# An example with a very long text\n",
    "from conspiracies.preprocessing.wordpiece_length_normalization import wordpiece_length_normalization\n",
    "\n",
    "long_text = [\"Hej mit navn er Kenneth. \" * 200]\n",
    "normalized_text = wordpiece_length_normalization(\n",
    "    long_text, nlp, tokenizer, max_length=500\n",
    ")\n",
    "normalized_text = list(normalized_text)\n",
    "assert len(normalized_text) > 1, \"a long text should be split into multiple texts\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction using Multi<sup>2</sup>OIE\n",
    "\n",
    "This pipeline uses a retrained version of the Multi<sup>2</sup>OIE model to extract relations from a text document. The model is based on the BERT architecture and uses multi-head attention to extract relations. To read more about the model and the dataset used to train it, please refer to the [paper](https://arxiv.org/abs/2009.08128v2).\n",
    "\n",
    "\n",
    "````{note}\n",
    "For this example we will use the spacy pipeline `en_core_web_sm` if you don't have it installed you can do so by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "spacy download en_core_web_sm\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/au479461/PycharmProjects/conspiracies/src/conspiracies/docprocessing/relationextraction/multi2oie/other/bio.py:209: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  return [torch.Tensor(pred_tags) for pred_tags in total_pred_tags]\n",
      "/home/au479461/PycharmProjects/conspiracies/src/conspiracies/docprocessing/relationextraction/multi2oie/other/bio.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(res, dtype=torch.bool, device=tensor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pernille Blume vinder delt EM-sølv i Ungarn. \n",
      " []\n",
      "Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri. \n",
      " [(i, disciplinen, 50 meter fri)]\n",
      "Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder. \n",
      " [(i, tiden, 23,97 sekunder)]\n",
      "Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona. \n",
      " []\n",
      "Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri. \n",
      " []\n",
      "In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call \n",
      " [(Mr. Biden, warned, Mr. Netanyahu), (he, could fend off, criticism of the Gaza strikes)]\n",
      "That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship. \n",
      " [(the fighting, started, last week), (That phone call and others since the fighting, reflect, Mr. Biden and Mr. Netanyahu's complicated 40-year relationship)]\n",
      "Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd. \n",
      " [(Politiet, skal etterforske, Siv Jensen etter mulig smittevernsbrudd)]\n",
      "En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist. \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "from conspiracies.docprocessing.relationextraction import SpacyRelationExtractor\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "test_sents = [\n",
    "    \"Pernille Blume vinder delt EM-sølv i Ungarn.\",\n",
    "    \"Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.\",\n",
    "    \"Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.\",\n",
    "    \"Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.\",\n",
    "    \"Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.\",\n",
    "    \"In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call\",\n",
    "    \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\",\n",
    "    \"Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.\",\n",
    "    \"En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.\",\n",
    "]\n",
    "\n",
    "\n",
    "# change these to your purposes. 2.7 is the default confidence threshold (the bulk of bad relations not kept and the majority of correct ones kept)\n",
    "# batch_size should be changed according to your device. Can most likely be bumped up a fair bit\n",
    "config = {\"confidence_threshold\": 2.7, \"model_args\": {\"batch_size\": 10}}\n",
    "nlp.add_pipe(\"relation_extractor\", config=config)\n",
    "\n",
    "pipe = nlp.pipe(test_sents)\n",
    "\n",
    "for d in pipe:\n",
    "    print(d.text, \"\\n\", d._.relation_triplets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation extraction without using SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pernille Blume vinder delt EM-sølv i Ungarn.', 'Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.', 'Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.', 'Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.', 'Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.', 'In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call', \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\", 'Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.', 'En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.']\n",
      "{'sentence': ['Pernille Blume vinder delt EM-sølv i Ungarn.', 'Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.', 'Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.', 'Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.', 'Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.', 'In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call', \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\", 'Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.', 'En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.'], 'wordpieces': [['[CLS]', 'Per', '##nil', '##le', 'Blume', 'vinde', '##r', 'delt', 'EM', '-', 'sølv', 'i', 'Ungarn', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'Per', '##nil', '##le', 'Blume', 'blev', 'nummer', 'to', 'ved', 'EM', 'på', 'lang', '##bane', 'i', 'discipline', '##n', '50', 'meter', 'fri', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'Hurt', '##ig', '##st', 'var', 'til', 'gen', '##g', '##æ', '##ld', 'holl', '##æ', '##nder', '##en', 'Ra', '##nomi', 'Kr', '##omo', '##wid', '##joj', '##o', ',', 'der', 'sikre', '##de', 'sig', 'guld', '##et', 'i', 'tiden', '23', ',', '97', 'sekunder', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'Og', 'at', 'formen', 'er', 'til', 'en', 'EM', '-', 'sølv', '##medalje', 'te', '##gner', 'godt', ',', 'sig', '##er', 'Per', '##nil', '##le', 'Blume', 'med', 'tank', '##e', 'på', ',', 'at', 'hun', 'få', 'ug', '##er', 'siden', 'var', 'sm', '##itte', '##t', 'med', 'corona', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'Ved', 'EM', 'tir', '##sdag', 'blev', 'det', 'ikke', 'til', 'medalje', 'for', 'den', 'danske', 'medley', 'for', 'mixed', '##e', 'hold', 'i', '4', 'x', '200', 'meter', 'fri', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'In', 'a', 'phone', 'call', 'on', 'Monday', ',', 'Mr', '.', 'Bi', '##den', 'war', '##ned', 'Mr', '.', 'Net', '##anya', '##hu', 'that', 'he', 'could', 'fe', '##nd', 'off', 'criticism', 'of', 'the', 'Gaza', 'strikes', 'for', 'only', 'so', 'long', ',', 'according', 'to', 'two', 'people', 'familiar', 'with', 'the', 'call', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'That', 'phone', 'call', 'and', 'others', 'since', 'the', 'fighting', 'started', 'last', 'week', 'reflect', 'Mr', '.', 'Bi', '##den', 'and', 'Mr', '.', 'Net', '##anya', '##hu', \"'\", 's', 'complicated', '40', '-', 'year', 'relationship', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'Pol', '##itie', '##t', 'skal', 'etter', '##fors', '##ke', 'Si', '##v', 'Jensen', 'etter', 'mulig', 'sm', '##itte', '##vern', '##s', '##bru', '##dd', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'En', 'av', 'Belgien', '##s', 'mest', 'fram', '##tr', '##ä', '##dande', 'virus', '##ex', '##pert', '##er', 'har', 'flytta', '##ts', 'med', 'sin', 'familj', 'till', 'sky', '##dda', '##t', 'bo', '##ende', 'efter', 'hot', 'från', 'en', 'be', '##vä', '##pna', '##d', 'hög', '##ere', '##xt', '##rem', '##ist', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']], 'confidence': [[2.4112268686294556, 0], [2.678802728652954, 2.802209436893463], [2.2977341413497925, 2.8315271735191345], [2.398663580417633, 1.4474357068538666], [2.4406053721904755], [4.736074388027191, 3.6018123626708984], [2.9236509203910828, 2.924257755279541], [2.9283658266067505], [2.205388695001602]], 'extraction_span': [[[[1, 2, 3, 4, 5, 6], [7], [8, 9, 10, 11, 12]], [[8, 9, 10], [], [11]]], [[[1, 2, 3, 4], [5], [6, 7, 8, 9, 10, 11, 12]], [[13], [14, 15], [16, 17, 18]]], [[[22], [23, 24], [25]], [[28], [29], [30, 31, 32, 33]]], [[[27], [28], [29, 30, 31, 32, 33, 34, 35, 36, 37]], [[29, 30, 31, 32], [33, 34, 35], [36, 37]]], [[[2, 3, 4], [5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]]], [[[8, 9, 10, 11], [12, 13], [14, 15, 16, 17, 18]], [[20], [21, 22, 23, 24], [25, 26, 27, 28, 29]]], [[[7, 8], [9], [10, 11]], [[1, 2, 3, 4, 5, 6, 7, 8], [12], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]], [[[1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]], [[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [14, 15, 16], [17, 18, 19, 21, 22, 23, 24, 25]]]], 'extraction': [[['Pernille Blume vinder', 'delt', 'EM-sølv i'], ['EM-sølv', '', 'i']], [['Pernille Blume', 'blev', 'nummer to ved EM på langbane'], ['i', 'disciplinen', '50 meter']], [['der', 'sikrede', 'sig'], ['i', 'tiden', '23,97']], [['hun', 'få', 'uger siden var smittet med'], ['uger siden var', 'smittet', 'med']], [['EM tirsdag', 'blev', 'det ikke til medalje for den danske medley for mixede hold']], [['Mr. Biden', 'warned', 'Mr. Netanyahu'], ['he', 'could fend off', 'criticism of the Gaza strikes']], [['the fighting', 'started', 'last week'], ['That phone call and others since the fighting', 'reflect', \"Mr. Biden and Mr. Netanyahu's complicated 40-year\"]], [['Politiet', 'skal etterforske', 'Siv Jensen etter mulig']], [['En av Belgiens mest framträdande virusexperter', 'har flyttats', 'med sin familj skyddat boende']]]}\n"
     ]
    }
   ],
   "source": [
    "from conspiracies.docprocessing.relationextraction import KnowledgeTriplets\n",
    "\n",
    "\n",
    "test_sents = [\n",
    "    \"Lasse er en dreng på 26 år.\",\n",
    "    \"Jeg arbejder som tømrer\",\n",
    "    \"Albert var videnskabsmand og døde i 1921\",\n",
    "    \"Lasse lives in Denmark and owns two cats\",\n",
    "]\n",
    "\n",
    "\n",
    "test_sents = [\n",
    "    \"Pernille Blume vinder delt EM-sølv i Ungarn.\",\n",
    "    \"Pernille Blume blev nummer to ved EM på langbane i disciplinen 50 meter fri.\",\n",
    "    \"Hurtigst var til gengæld hollænderen Ranomi Kromowidjojo, der sikrede sig guldet i tiden 23,97 sekunder.\",\n",
    "    \"Og at formen er til en EM-sølvmedalje tegner godt, siger Pernille Blume med tanke på, at hun få uger siden var smittet med corona.\",\n",
    "    \"Ved EM tirsdag blev det ikke til medalje for den danske medley for mixede hold i 4 x 200 meter fri.\",\n",
    "    \"In a phone call on Monday, Mr. Biden warned Mr. Netanyahu that he could fend off criticism of the Gaza strikes for only so long, according to two people familiar with the call\",\n",
    "    \"That phone call and others since the fighting started last week reflect Mr. Biden and Mr. Netanyahu's complicated 40-year relationship.\",\n",
    "    \"Politiet skal etterforske Siv Jensen etter mulig smittevernsbrudd.\",\n",
    "    \"En av Belgiens mest framträdande virusexperter har flyttats med sin familj till skyddat boende efter hot från en beväpnad högerextremist.\",\n",
    "]\n",
    "\n",
    "\n",
    "# initialize a class object\n",
    "# call the class method for extracting triplets from a given list of sentences\n",
    "\n",
    "relations = KnowledgeTriplets()\n",
    "final_result = relations.extract_relations(test_sents)\n",
    "\n",
    "\n",
    "print(final_result[\"sentence\"])\n",
    "print(final_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with Conspiracies\n",
    "\n",
    "\n",
    "```python\n",
    "from conspiracies.data import load_gold_triplets\n",
    "\n",
    "# load data\n",
    "docs, triplets = load_gold_triplets()\n",
    "\n",
    "print(docs[0])\n",
    "print(\"------\\n\\n\")\n",
    "for triplet in triplets[0]:\n",
    "    triplet.visualize()\n",
    "```\n",
    "Output is redacted for privacy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"da\")\n",
    "\n",
    "api_key = \"\"  # your api key\n",
    "config = {\n",
    "    \"prompt_template\": \"conspiracies/template_1\",\n",
    "    \"examples\": None,\n",
    "    \"task_description\": None,\n",
    "    \"model_name\": \"text-davinci-002\",\n",
    "    \"backend\": \"conspiracies/openai_gpt3_api\",\n",
    "    \"api_key\": api_key,\n",
    "    \"split_doc_fn\": None,\n",
    "    \"api_kwargs\": {\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "    },\n",
    "    \"force\": True,\n",
    "}\n",
    "\n",
    "relation_component = nlp.add_pipe(\n",
    "    \"conspiracies/prompt_relation_extraction\", last=True, config=config\n",
    ")\n",
    "\n",
    "# If you want to add examples to the prompt\n",
    "# you can do it in the following way\n",
    "# prompt_template = relation_component.prompt_template\n",
    "# prompt_template.examples = examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the pipeline to a single document\n",
    "\n",
    "```python\n",
    "doc = nlp(\"This is an example of a target tweet\")\n",
    "for triplet in doc._.relation_triplets:\n",
    "    print(\" -\", triplet)\n",
    "```\n",
    "Which should print:\n",
    "```\n",
    "- subject=This predicate=is object=an example of a target tweet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
